{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511deb2-0c4e-4c49-b43b-7f0797712597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, \n",
    "and what does it represent?\n",
    "ANS.R squared (R2  ) value in machine learning is referred to as the coefficient of \n",
    "   determination or the coefficient of multiple determination in case of multiple regression.  \n",
    ":  R squared in regression acts as an evaluation metric to evaluate the scatter \n",
    "   of the data points around the fitted regression line. It recognizes the percentage \n",
    "   of variation of the dependent variable. \n",
    " \n",
    ": R-squared is the proportion of variance in the dependent variable that can be explained\n",
    "  by the independent variable.\n",
    "  R squared =(SSregression)/(SStotal)\n",
    "    var(u) = 1/n∑(ui – ū)2\n",
    "    The value of R-squared stays between 0 and 100%.\n",
    "    \n",
    ":  Visual demonstration of the plots of fitted values by observed values in a graphical manner. \n",
    "   It illustrates how R-squared values represent the scatter around the regression line\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n",
    "Ans:Adjusted R-squared is a modified version of R-squared that has been adjusted for the \n",
    "    number of predictors in the model. The adjusted R-squared increases when the new term improves\n",
    "     the model more than would be expected by chance. It decreases when a predictor improves the \n",
    "     model by less than expected.\n",
    ":    The predicted R-squared, unlike the adjusted R-squared, is used to indicate how well a \n",
    "     regression model predicts responses for new observations. So where the adjusted R-squared\n",
    "     can provide an accurate model that fits the current data, the predicted R-squared determines \n",
    "     how likely it is that this model will be accurate for future data.\n",
    "    \n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Ans.Adjusted R2 is the better model when you compare models that have a different amount\n",
    "of variables. The logic behind it is, that R2 always increases when the number of variables\n",
    "increases.\n",
    "Meaning that even if you add a useless variable to you model, your R2 will still increase.\n",
    ":The most obvious difference between adjusted R-squared and R-squared is simply that adjusted\n",
    "R-squared considers and tests different independent variables against the stock index and R-squared does not. \n",
    "Because of this, many investment professionals prefer using adjusted R-squared because it has \n",
    "the potential to be more accurate.\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "Ans:\n",
    "RMSE:This article was published as a part of the Data Science Blogathon\n",
    "The objective of any machine learning model is to understand and learn patterns from\n",
    "the data which can further be used to make predictions or answer questions or simply\n",
    "just understand the underlying pattern that is otherwise not evident candidly. Most of the time,\n",
    "the learning part is iterative. A model learns some patterns from the data, we test it against\n",
    "some new data that the model did not encounter during training, we see how good or how bad a \n",
    "job it did, we tweak and adjust some parameters, then we put it to test again.\n",
    "This process is repeated until we are presented with a model that is good enough \n",
    "some real-world models can just be satisfactory and make a world of difference).\n",
    "The part in which we evaluate and test our model is where the loss functions come into play. \n",
    "Evaluation metric is an integral part of regression models.\n",
    "Loss functions take the model’s predicted values and compare them against the actual values.\n",
    "\n",
    "MSE:MSE is one of the most common regression loss functions. In Mean Squared Error also\n",
    "known as L2 loss, we calculate the error by squaring the difference between the predicted \n",
    "value and actual value and averaging it across the dataset. MSE is also known as Quadratic\n",
    "loss as the penalty is not proportional to the error but to the square of the error.\n",
    ": MSE will never be negative since the errors are squared. The value of the error ranges from\n",
    "zero to infinity. MSE increases exponentially with an increase in error. \n",
    "A good model will have an MSE value closer to zero.\n",
    "\n",
    "MAE:Mean absolute error, also known as L1 loss is one of the simplest loss functions and an\n",
    "easy-to-understand evaluation metric. It is calculated by taking the absolute difference\n",
    "between the predicted values and the actual values and averaging it across the dataset.\n",
    "it is the arithmetic average of absolute errors. MAE measures only the magnitude of the errors\n",
    "and doesn’t concern itself with their direction. The lower the MAE, the higher the accuracy of \n",
    "a model.\n",
    "\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "Ans-\n",
    "MAE\n",
    "  ADVANTAGE ::\n",
    "         It is an easy to calculate evaluation metric.\n",
    "         A ll the errors are weighted on the same scale since absolute values are taken.\n",
    "         It is useful if the training data has outliers as MAE does not penalize high errors\n",
    "         caused by outliers.\n",
    "         It provides an even measure of how well the model is performing.\n",
    " DISADVANTAGE ::\n",
    "         Sometimes the large errors coming from the outliers end up being treated as the same as low errors.\n",
    "         MAE follows a scale-dependent accuracy measure where it uses the same scale as the data \n",
    "         being measured. \n",
    "         Hence it cannot be used to compare series’ using different measures.\n",
    "         One of the main disadvantages of MAE is that it is not differentiable at zero. \n",
    "         Many optimization algorithms tend to use differentiation to find the optimum value\n",
    "         for parameters in the evaluation metric.\n",
    "         It can be challenging to compute gradients in MAE.\n",
    " \n",
    "    MSE::\n",
    "    ADVANTAGE::\n",
    "               MSE values are expressed in quadratic equations. Hence when we plot it, we get a \n",
    "               gradient descent with only one global minima.\n",
    "               For small errors, it converges to the minima efficiently. There are no local minima.\n",
    "               MSE penalizes the model for having huge errors by squaring them.\n",
    "               It is particularly helpful in weeding out outliers with large errors from the model by\n",
    "               putting more weight on them\n",
    "\n",
    " DISADVANTAGE:::\n",
    "        One of the advantages of MSE becomes a disadvantage when there is a bad prediction. \n",
    "        The sensitivity to outliers magnifies the high errors by squaring them.\n",
    "        MSE will have the same effect for a single large error as too many smaller errors. But mostly we \n",
    "        will be looking for a model which performs well enough on an overall level.\n",
    "        MSE is scale-dependent as its scale depends on the scale of the data. This makes it highly \n",
    "        undesirable for comparing different measures.\n",
    "\n",
    "RMSE::\n",
    "    ADVANTAGE::\n",
    "        RMSE is easy to understand.\n",
    "        It serves as a heuristic for training models.\n",
    "        It is computationally simple and easily differentiable which many optimization algorithms desire.\n",
    "         RMSE does not penalize the errors as much as MSE does due to the square root.\n",
    "  \n",
    "   DISADVANTAGE::\n",
    "           Like MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.\n",
    "           One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function properly.\n",
    "           RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the results on different\n",
    "           test samples\n",
    "\n",
    "\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, \n",
    "and when is it more appropriate to use?\n",
    "ANS-Lasso regression stands for Least Absolute Shrinkage and Selection Operator. \n",
    "It adds penalty term to the cost function. This term is the absolute sum of the coefficients. \n",
    "As the value of coefficients increases from 0 this term penalizes, cause model, to decrease\n",
    "the value of coefficients in order to reduce loss. The difference between ridge and lasso \n",
    "regression is that it tends to make coefficients to \n",
    "absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.\n",
    ":Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients\n",
    "by introducing a penalty factor. However, while lasso regression takes the magnitude of the\n",
    "coefficients,\n",
    "ridge regression takes the square. Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "ANS:Model overfitting is a serious problem and can cause the model to produce misleading \n",
    "information. One of the techniques to overcome overfitting is Regularization. Regularization,\n",
    "in general, penalizes the coefficients that cause the overfitting of the model. \n",
    "There are two norms in regularization that can be used as per the scenarios.\n",
    "ex-\n",
    "lasso.fit(x_train, y_train)\n",
    "WE USE for overfitting\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "Ans:During the Machine Learning model building, the Regularization Techniques is an\n",
    "unavoidable and important step to improve the model prediction and reduce errors.\n",
    "This is also called the Shrinkage method. Which we use to add\n",
    "the penalty term to control the complex model to avoid overfitting by reducing the variance.\n",
    ":Due to the too many Dimensions with too few data, the algorithm would build the best fit \n",
    "with peaks and deep-down dells in the observation along with the high magnitude of coefficient \n",
    "its leads to overfitting and is not suitable for production. \n",
    "drastic fluctuation in surface inclination.\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "ans.Overall model performance is more important than model accuracy, \n",
    "since model accuracy only takes one metric into account.\n",
    "Even when model fails to predict any Crashes its accuracy is still 90%. As data contain\n",
    "90% Landed Safely. So, accuracy does not holds good for imbalanced data. In business scenarios,\n",
    "most data won't \n",
    "be balanced and so accuracy becomes poor measure of evaluation for our classification model\n",
    "\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    " \n",
    "Ans: The mean of predicted values of 0.5 API is calculated by taking the sum of the predicted values\n",
    "for 0.5 API divided by the total number of samples having 0.5 API.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a883e7-6f32-457a-8c55-01483b198988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e71a0-1dd3-4262-9084-d76be006324d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfaec9-9368-402d-8464-07b1fa219666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd897607-1722-4c54-bc45-bf2af9820be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b04a73-032b-4e58-8b75-d611d3dc724d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
