{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e988e00-2325-45dc-8f42-1482cfeff5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "ans-Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator.\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "ans-Assumptions of Ridge Regressions\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "ans-Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "ans-Unlike lasso, ridge does not have zeroing coefficients as a goal, and you shouldn't expect applying ridge penalty to have this effect. So the answer to your title question is \"no.\"\n",
    "We can use ridge regression for feature selection while fitting the model.\n",
    "Feature selection is a way to reduce the number of features and hence reduce the computational complexity of the model. Many times feature selection becomes very useful to overcome with overfitting problem.\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "ans-Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in regression estimates significantly increases the reliability of the estimates.\n",
    "When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "ans-Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous. So for your analysis Ridge regression can't be used.\n",
    "Models in which there are both continuous and categorical variables are often called analysis of covariance or ANCOVA for short.\n",
    "For example, linear regression is used when the dependent variable is continuous, logistic regression when the dependent is categorical with 2 categories, and multinominal regression when the dependent is categorical with more than 2 categories.\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "ans-A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease.\n",
    "The ridge estimate is given by the point at which the ellipse and the circle touch. There is a trade-off between the penalty term and RSS. Maybe a large would give you a better residual sum of squares but then it will push the penalty term higher\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "ans-The ridge regression technique can be used to predict time-series. Ridge regression (RR) can also solve the multicollinearity problem that exists in linear regression.\n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
