{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fce794-bc3f-49c1-a3a9-10fe03134e69",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd1ec2-5e01-4cae-bb12-b3a74f17cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression (GBR) is a popular machine learning technique for solving regression problems. It is an ensemble method that combines multiple weak learners, typically decision trees, to create a powerful model\n",
    ". GBR works by sequentially adding decision trees to the model,\n",
    "with each new tree attempting to correct the errors made by the\n",
    "previous trees.\n",
    "\n",
    "In GBR, the model is trained to minimize a loss function, such \n",
    "as mean squared error, between the predicted values and the\n",
    "actual values. The algorithm starts by fitting a single \n",
    "decision tree to the data and computing the residuals, which \n",
    "are the differences between the predicted values and the actual\n",
    "values. The next decision tree is then fit to the residuals, a\n",
    "nd this process is repeated until a predefined stopping \n",
    "criterion is met, such as reaching a maximum number of trees \n",
    "or achieving a desired level of performance.\n",
    "\n",
    "GBR is known for its high predictive accuracy and ability \n",
    "to handle complex non-linear relationships in the data.\n",
    "However, it can be prone to overfitting if the number of \n",
    "trees is too high or the learning rate is too low, and it\n",
    "can be computationally expensive to train on large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc748bcb-3dd9-40ad-b325-c5b48ff81344",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8068ae2-98c7-4a6a-b321-aba8520013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the y_pred as the mean of y\n",
    "        y_pred = np.full(y.shape, np.mean(y))\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - y_pred\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = RegressionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            # Update y_pred by adding the predictions of the current tree multiplied by the learning rate\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            # Add the current tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions as the mean of y\n",
    "        y_pred = np.full(X.shape[0], np.mean(y))\n",
    "        for tree in self.estimators:\n",
    "            # Update the predictions by adding the predictions of the current tree multiplied by the learning rate\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "class RegressionTree:\n",
    "    \n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Base case: stop splitting when we reach the maximum depth or when there is only one sample\n",
    "        if depth == self.max_depth or n_samples == 1:\n",
    "            return LeafNode(y)\n",
    "        # Split the data into two subsets based on the best feature and split point\n",
    "        feature_idx, split_point = self._find_best_split(X, y)\n",
    "        left_mask = X[:, feature_idx] <= split_point\n",
    "        right_mask = X[:, feature_idx] > split_point\n",
    "        # Recursive case: build a tree for each subset\n",
    "        left_node = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_node = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        return InternalNode(feature_idx, split_point, left_node, right_node)\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        best_feature_idx = None\n",
    "        best_split_point = None\n",
    "        min_mse = np.inf\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            for split_point in feature_values:\n",
    "                left_mask = feature_values <= split_point\n",
    "                right_mask = feature_values > split_point\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "                mse = ((left_y - np.mean(left_y))**2).sum() + ((right_y - np.mean(right_y))**2).sum()\n",
    "                if mse < min_mse:\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_split_point = split_point\n",
    "                    min_mse = mse\n",
    "        return best_feature_idx, best_split_point\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.root.predict(x) for x in X])\n",
    "    \n",
    "class InternalNode:\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c997f-2081-4202-8702-ecd2a583e0df",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc787a2-8e36-4421-be8d-6ae59f334c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a random regression problem\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor object\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Use grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cab188-95af-497a-8989-4f8498e13b9a",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361f403-a428-4a6e-a8e1-dfb373d2ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner is a simple model \n",
    "or an algorithm that can only make a slightly better\n",
    "prediction than random guessing on its own. These are\n",
    "typically decision trees with a small number of nodes, \n",
    "also known as decision stumps, that are used to model the \n",
    "residual errors or the gradient of the loss function. \n",
    "In Gradient Boosting, the weak learners are trained in a\n",
    "sequential manner, where each subsequent weak learner tries \n",
    "to improve the prediction errors of the previous one. The final\n",
    "model is a combination of these weak learners, which results in\n",
    "a much stronger and more accurate predictor than the individual\n",
    "weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075884cc-b6f8-47bf-ae2a-4e0dc02f8b66",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118807e6-18d0-494d-864b-6bad5c06c3e5",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27de1a3-ce59-4897-a00d-f914fa75c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to combine multiple weak learners into a strong learner\n",
    "in an iterative manner. At each iteration, the algorithm \n",
    "fits a new weak learner to the residual errors from the\n",
    "previous iteration. By doing so, the algorithm learns to \n",
    "correct the errors made by the previous weak learners, \n",
    "and the resulting model gradually improves in performance.\n",
    "\n",
    "The algorithm uses a gradient descent optimization technique \n",
    "to minimize the loss function, which measures the difference\n",
    "between the predicted and actual values. By taking the gradient \n",
    "f the loss function with respect to the predictions, \n",
    "the algorithm can update the model in a direction that \n",
    "reduces the loss. The learning rate hyperparameter controls \n",
    "the step size of each update, and the number of iterations\n",
    "determines how many weak learners are combined to form the\n",
    "final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9e05f-0074-445d-9a58-5051cbc5db7b",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16126be3-170b-49d9-8942-17de184b41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting algorithm builds an ensemble of weak\n",
    "learners in a sequential manner. At each iteration, the \n",
    "algorithm fits a weak learner to the negative gradient of\n",
    "the loss function with respect to the previous ensemble's\n",
    "output. The weak learner is trained to predict the negative \n",
    "gradient values of the previous ensemble output as targets.\n",
    "\n",
    "\n",
    "Then, the output of the weak learner is added to the previous\n",
    "ensemble's output, and the algorithm repeats the process until\n",
    "the specified number of weak learners is reached. Each weak\n",
    "learner contributes a small part to the final prediction, \n",
    "and the algorithm combines them to create a powerful ensemble\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357a28a-31fe-446e-9f91-9a2b32cb23e6",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161b5b9-6a6d-4b42-9767-4bbc177c3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm can be constructed through the following steps:\n",
    "\n",
    "1. Define the loss function: The first step is to define a differentiable loss function that measures the error between the predicted and actual values. A common choice for regression problems is the mean squared error (MSE) loss.\n",
    "\n",
    "2. Initialize the model: The second step is to initialize the model with a constant value, typically the mean of the target variable.\n",
    "\n",
    "3. Fit a weak learner: The third step is to fit a weak learner, such as a decision tree, to the residual errors of the model.\n",
    "\n",
    "4. Compute the optimal step size: The fourth step is to compute the optimal step size or learning rate that minimizes the loss function when the weak learner is added to the model. This can be done through numerical optimization techniques such as gradient descent.\n",
    "\n",
    "5. Update the model: The fifth step is to update the model by adding the weak learner with the computed step size.\n",
    "\n",
    "6. Repeat steps 3-5: The process is then repeated by fitting another weak learner to the new residual errors and updating the model until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "7. Make predictions: Finally, the ensemble of weak learners is used to make predictions on new data by summing the predictions of each weak learner, weighted by the step size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b7242-3234-4f6d-803c-2835b35dd23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feaf67e-ae06-4a47-9a45-ff0267150371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
