{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872644c-f6b5-434c-94a4-5168322bcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "ANS-The mathematical formula for a linear SVM is:\n",
    "f(x) = sign(w^T * x + b)\n",
    "where:- f(x) is the predicted class label for the input data point x,\n",
    "- sign() is the sign function that returns +1 or -1 depending on the sign of its argument,\n",
    "- w is the weight vector that defines the separating hyperplane, and\n",
    "- b is the bias term.\n",
    "The separating hyperplane is defined as the set of all points x in the feature space for which the inner product w^T * x + b is equal to zero.\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "ANS-The objective function of a linear SVM is to maximize the margin, which is the distance between the separating hyperplane and the closest data points from each class.\n",
    "Mathematically, the objective function of a linear SVM can be formulated as:minimize: 1/2 * ||w||^2subject to: y_i(w^T * x_i + b) >= 1where:- w is the weight vector that defines the separating hyperplane,\n",
    "- b is the bias term,\n",
    "- ||w|| is the L2 norm of the weight vector,\n",
    "- y_i is the class label (+1 or -1) of the i-th training example,\n",
    "- x_i is the i-th training example, and\n",
    "- the inequality constraint ensures that all data points are correctly classified by the separating hyperplane, with a margin of at least 1.This is a convex optimization problem that can be solved using quadratic programming methods. The solution to the optimization problem is the weight vector w and the bias term b, which define the separating hyperplane.\n",
    "Q3. What is the kernel trick in SVM?\n",
    "ANS-The kernel trick in SVM is a technique that allows the SVM algorithm to efficiently handle non-linearly separable datasets by implicitly mapping the original input data to a higher-dimensional feature space where the data points are more likely to be linearly separable.\n",
    "\n",
    "The kernel trick works by introducing a kernel function K(x_i, x_j) that computes the inner product of the mapped feature vectors of two input data points, without explicitly computing the mapping. In other words, instead of computing the dot product of the original input vectors, the kernel function computes the dot product of the mapped feature vectors. This allows the SVM algorithm to operate in the high-dimensional feature space without actually computing the mapped feature vectors, which can be computationally expensive.\n",
    "\n",
    "The most commonly used kernel functions in SVM are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel. The choice of kernel function depends on the characteristics of the data and the problem at hand.\n",
    "\n",
    "The kernel trick is a powerful technique that has significantly improved the performance of SVM on many real-world datasets, especially in natural language processing, computer vision, and bioinformatics.\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "ANS-Support vectors are the data points that lie closest to the decision boundary (hyperplane) in a Support Vector Machine (SVM) model. These data points play a critical role in defining the decision boundary and maximizing the margin between the two classes. \n",
    "\n",
    "In SVM, the objective is to find the hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the decision boundary and the closest data points from each class. These closest data points are known as support vectors. \n",
    "\n",
    "The role of support vectors is to define the decision boundary of the SVM model. The SVM algorithm tries to find the hyperplane that maximizes the margin between the two classes while still correctly classifying all the training data. The support vectors are the data points that determine the position of the hyperplane and hence the decision boundary. \n",
    "\n",
    "For example, consider a binary classification problem with two classes, A and B. We have a dataset consisting of ten data points, five in class A and five in class B. Suppose the SVM algorithm finds the decision boundary that separates the two classes as shown in the figure below:\n",
    "\n",
    "![svm_example](https://i.imgur.com/KwhJ0M8.png)\n",
    "\n",
    "In this example, the support vectors are the three data points that lie closest to the decision boundary. These points are circled in red. The support vectors determine the position of the decision boundary and hence the margin between the two classes. In other words, if we remove any of these support vectors, the position of the decision boundary will change, and the margin will decrease. Therefore, these support vectors are critical for defining the decision boundary and maximizing the margin.\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "ANS-Sure, here are the explanations and graphs for Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM:\n",
    "\n",
    "1. Hyperplane: In SVM, a hyperplane is a decision boundary that separates two classes. The hyperplane can be linear or non-linear, depending on the type of kernel used. For example, in a two-dimensional space, a linear hyperplane can be represented by a line while in a three-dimensional space, a linear hyperplane can be represented by a plane.\n",
    "\n",
    "2. Marginal plane: The marginal plane is a hyperplane that runs parallel to the separating hyperplane and is equidistant from the two nearest points of the two classes. The distance between the marginal plane and the separating hyperplane is called the margin. The margin is maximized in order to increase the robustness of the SVM model to new data points.\n",
    "\n",
    "3. Soft margin: In the case where the classes are not perfectly separable, we can use a soft margin SVM. The soft margin SVM allows for some misclassification in order to increase the robustness of the model. The soft margin SVM adds a slack variable to the optimization problem to allow for some misclassification. The objective of the optimization problem is to minimize the misclassification error and the size of the margin.\n",
    "\n",
    "4. Hard margin: In the case where the classes are perfectly separable, we can use a hard margin SVM. The hard margin SVM does not allow for any misclassification and tries to find a hyperplane that perfectly separates the two classes.\n",
    "\n",
    "Here are the graphical representations of the above concepts:\n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "![Hyperplane](https://i.imgur.com/FI7Mx4e.png)\n",
    "\n",
    "In the above image, the hyperplane is represented by the line that separates the two classes.\n",
    "\n",
    "2. Marginal plane:\n",
    "\n",
    "![Marginal plane](https://i.imgur.com/jAjqfOm.png)\n",
    "\n",
    "In the above image, the marginal plane is represented by the dashed lines that run parallel to the separating hyperplane and are equidistant from the two nearest points of the two classes.\n",
    "\n",
    "3. Soft margin:\n",
    "\n",
    "![Soft margin](https://i.imgur.com/wLYRg4X.png)\n",
    "\n",
    "In the above image, the soft margin SVM allows for some misclassification and adds a slack variable to the optimization problem to allow for some misclassification. The objective of the optimization problem is to minimize the misclassification error and the size of the margin.\n",
    "\n",
    "4. Hard margin:\n",
    "\n",
    "![Hard margin](https://i.imgur.com/9VHlLpN.png)\n",
    "\n",
    "In the above image, the hard margin SVM does not allow for any misclassification and tries to find a hyperplane that perfectly separates the two classes. The margin is maximized to increase the robustness of the SVM model to new data points.\n",
    "\n",
    "\n",
    "Q6..SVM Implementation through Iris dataset.\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "ANS-from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "clf = LinearSVC(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the features\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()\n",
    "\n",
    "# Try different values of the regularisation parameter C and see how it affects the performance of the model\n",
    "C_values = [0.1, 1, 10]\n",
    "for C in C_values:\n",
    "    clf = LinearSVC(C=C, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for C={}: {}\".format(C, accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
